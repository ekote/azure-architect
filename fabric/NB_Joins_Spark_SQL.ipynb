{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "kernelspec": {
   "name": "synapse_pyspark",
   "display_name": "Synapse PySpark"
  },
  "microsoft": {
   "language": "python",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   },
   "host": {
    "synapse_widget": {
     "token": "3592e938-2ff1-490d-8540-24d6714cf2d9",
     "state": {}
    }
   }
  },
  "widgets": {},
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "save_output": true,
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "enableDebugMode": false,
    "conf": {}
   }
  },
  "notebook_environment": {},
  "synapse_widget": {
   "version": "0.1",
   "state": {}
  },
  "trident": {
   "lakehouse": {
    "default_lakehouse": "be6aaade-86ef-42f1-8737-a6f25e807137",
    "known_lakehouses": [
     {
      "id": "be6aaade-86ef-42f1-8737-a6f25e807137"
     }
    ],
    "default_lakehouse_name": "Bronze",
    "default_lakehouse_workspace_id": "02ce6c99-9dde-4918-9a38-d95b612d774e"
   }
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "from pyspark.sql.functions import col, avg, max, when\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, avg, when\n",
    "import time\n",
    "import random\n",
    "import string\n",
    "\n",
    "def generate_table_name(length=10):\n",
    "    letters = string.ascii_lowercase\n",
    "    table_name = ''.join(random.choice(letters) for i in range(length))\n",
    "    return table_name\n",
    "\n",
    "def clean_tables(table_name):\n",
    "    for table in table_name:\n",
    "        q = '''\n",
    "        drop table if exists '''+ table\n",
    "        spark.sql(q)\n",
    "\n",
    "\n",
    "def queries(pause: bool):\n",
    "    tables = []\n",
    "\n",
    "    % TODO parametrize your delta table e.g., name_of_your_lakehouse.delta_table_name\n",
    "    % true_big_table is based on yellow_tripdata_2015-01.csv\n",
    "\n",
    "    print(\"Scenario 1\")\n",
    "    q = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        Year, \n",
    "        VendorID, \n",
    "        total_fare \n",
    "    FROM \n",
    "        (SELECT \n",
    "            'Year1' as Year, \n",
    "            VendorID, \n",
    "            SUM(fare_amount) AS total_fare,\n",
    "            ROW_NUMBER() OVER (ORDER BY SUM(fare_amount) DESC) as row_num \n",
    "        FROM Bronze.true_big_table \n",
    "        GROUP BY VendorID) \n",
    "    WHERE row_num = 1\n",
    "\n",
    "    UNION ALL\n",
    "\n",
    "    SELECT \n",
    "        Year, \n",
    "        VendorID, \n",
    "        total_fare \n",
    "    FROM \n",
    "        (SELECT \n",
    "            'Year2' as Year, \n",
    "            VendorID, \n",
    "            SUM(fare_amount) AS total_fare,\n",
    "            ROW_NUMBER() OVER (ORDER BY SUM(fare_amount) DESC) as row_num \n",
    "        FROM Bronze.broadcast_true_big_table_52402 \n",
    "        GROUP BY VendorID) \n",
    "    WHERE row_num = 1\n",
    "    \"\"\")\n",
    "\n",
    "    table_name = generate_table_name()\n",
    "    q.write.format(\"delta\").mode(\"overwrite\").saveAsTable(table_name)\n",
    "    tables.append(table_name)\n",
    "    \n",
    "\n",
    "    # around 10 s\n",
    "    print(\"Scenario 2\")\n",
    "    q = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        'Year1' as Year, \n",
    "        payment_type, \n",
    "        AVG(tip_amount) AS avg_tip \n",
    "    FROM Bronze.true_big_table \n",
    "    GROUP BY payment_type\n",
    "    UNION\n",
    "    SELECT \n",
    "        'Year2' as Year, \n",
    "        payment_type, \n",
    "        AVG(tip_amount) AS avg_tip \n",
    "    FROM Bronze.broadcast_true_big_table_52402 \n",
    "    GROUP BY payment_type\n",
    "    \"\"\")\n",
    "    table_name = generate_table_name()\n",
    "    q.write.format(\"delta\").mode(\"overwrite\").saveAsTable(table_name)\n",
    "    tables.append(table_name)\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(\"Scenario 8\")\n",
    "    q = spark.sql(\"\"\"\n",
    "    SELECT t1.VendorID, t1.avg_fare, t2.max_fare\n",
    "    FROM\n",
    "        (SELECT VendorID, AVG(fare_amount) AS avg_fare \n",
    "        FROM Bronze.true_big_table \n",
    "        GROUP BY VendorID) t1\n",
    "    JOIN \n",
    "        (SELECT VendorID, MAX(fare_amount) AS max_fare \n",
    "        FROM Bronze.broadcast_true_big_table_52402 \n",
    "        GROUP BY VendorID) t2\n",
    "    ON t1.VendorID = t2.VendorID\n",
    "    \"\"\")\n",
    "    q.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"dpscenario8\")\n",
    "\n",
    "    print(\"Scenario 8 extra \")\n",
    "    q = spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM Bronze.true_big_table\n",
    "    WHERE VendorID IN (SELECT VendorID FROM Bronze.broadcast_true_big_table_52402)\n",
    "    \"\"\")\n",
    "    table_name = generate_table_name()\n",
    "    q.write.format(\"delta\").mode(\"overwrite\").saveAsTable(table_name)\n",
    "    tables.append(table_name)\n",
    "\n",
    "\n",
    "    print(\"Scenario 9\")\n",
    "    q = spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM Bronze.true_big_table\n",
    "    WHERE VendorID NOT IN (SELECT VendorID FROM Bronze.broadcast_true_big_table_52402)\n",
    "    \"\"\")\n",
    "    table_name = generate_table_name()\n",
    "    q.write.format(\"delta\").mode(\"overwrite\").saveAsTable(table_name)\n",
    "    tables.append(table_name)\n",
    "\n",
    "\n",
    "    print(\"Scenario 10\")\n",
    "    q = spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM Bronze.true_big_table\n",
    "    CROSS JOIN Bronze.broadcast_true_big_table_52402\n",
    "    \"\"\")\n",
    "    # table_name = generate_table_name()\n",
    "    # q.write.format(\"delta\").mode(\"overwrite\").saveAsTable(table_name)\n",
    "    # tables.append(table_name)\n",
    "    q.count()\n",
    "\n",
    "\n",
    "    print(\"Scenario 11\")\n",
    "    q = spark.sql(\"\"\"\n",
    "    SELECT /*+ BROADCAST(t1) */ *\n",
    "    FROM Bronze.broadcast_true_big_table_52402 t1\n",
    "    JOIN Bronze.broadcast_true_big_table_52402 t2\n",
    "    ON t1.VendorID = t2.VendorID\n",
    "    \"\"\")\n",
    "    table_name = generate_table_name()\n",
    "    q.write.format(\"delta\").mode(\"overwrite\").saveAsTable(table_name)\n",
    "    tables.append(table_name)\n",
    "\n",
    "    time.sleep(200) if pause else time.sleep(1)\n",
    "\n",
    "    clean_tables(tables)\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": false,
     "outputs_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "id": "6aec4496-072e-40eb-9c43-ab74d24fd14d"
  },
  {
   "cell_type": "code",
   "source": [
    "queries(pause = False)"
   ],
   "outputs": [],
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": false,
     "outputs_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "id": "c873d0b1-49d1-4167-b881-de5c278d2cd8"
  }
 ]
}
