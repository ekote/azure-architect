{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "microsoft": {
   "language": "python",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   },
   "host": {
    "synapse_widget": {
     "token": "d4480cb7-6440-4ba7-9de1-58cd539132ed",
     "state": {}
    }
   }
  },
  "widgets": {},
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "kernelspec": {
   "name": "synapse_pyspark",
   "display_name": "Synapse PySpark"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "save_output": true,
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "enableDebugMode": false,
    "conf": {}
   }
  },
  "notebook_environment": {},
  "synapse_widget": {
   "version": "0.1",
   "state": {}
  },
  "trident": {
   "lakehouse": {
    "default_lakehouse": "be6aaade-86ef-42f1-8737-a6f25e807137",
    "known_lakehouses": [
     {
      "id": "be6aaade-86ef-42f1-8737-a6f25e807137"
     }
    ],
    "default_lakehouse_name": "Bronze",
    "default_lakehouse_workspace_id": "02ce6c99-9dde-4918-9a38-d95b612d774e"
   }
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# EDA Spark SQL\n",
    "by Estera Kot"
   ],
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "id": "4479ed99-2285-40da-97d7-d69fa66bd86b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import time\n",
    "from pyspark.sql.functions import col, avg, max, when\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, avg, when\n",
    "import time\n",
    "import random\n",
    "import string\n",
    "\n",
    "def generate_table_name(length=10):\n",
    "    letters = string.ascii_lowercase\n",
    "    table_name = ''.join(random.choice(letters) for i in range(length))\n",
    "    return table_name\n",
    "\n",
    "def clean_tables(table_name):\n",
    "    for table in table_name:\n",
    "        q = '''\n",
    "        drop table if exists '''+ table\n",
    "        spark.sql(q)\n",
    "\n",
    "def adjust_columns_names(df):\n",
    "    for col_name in df.columns:\n",
    "        # Create a new column name by removing underscores and parentheses\n",
    "        new_col_name = col_name.replace(\"_\", \"\").replace(\"(\", \"\").replace(\")\", \"\")\n",
    "        \n",
    "        # Rename the column\n",
    "        df = df.withColumnRenamed(col_name, new_col_name)\n",
    "        return df\n",
    "\n",
    "\n",
    "def queries(pause: bool):\n",
    "    tables = []\n",
    "\n",
    "    % TODO parametrize your delta table e.g., name_of_your_lakehouse.delta_table_name\n",
    "    df = spark.sql(\"SELECT * FROM Bronze.yellowtripdata201501\")\n",
    "    df.count()\n",
    "\n",
    "    print(\"Step 1\")    \n",
    "    segments = [0, 5, 10, 15, 20, 25]\n",
    "    query = \"\"\"\n",
    "    SELECT *,\n",
    "    CASE\n",
    "        WHEN trip_distance IS NULL THEN NULL\n",
    "        WHEN trip_distance <= {} THEN \"<= {}\"\n",
    "        WHEN trip_distance <= {} THEN \"{} - {}\"\n",
    "        WHEN trip_distance <= {} THEN \"{} - {}\"\n",
    "        WHEN trip_distance <= {} THEN \"{} - {}\"\n",
    "        WHEN trip_distance <= {} THEN \"{} - {}\"\n",
    "        ELSE \">= {}\"\n",
    "    END AS distance_segment\n",
    "    FROM Bronze.true_big_table\n",
    "    \"\"\".format(segments[0], segments[0], \n",
    "            segments[1], str(segments[0]+0.01), str(segments[1]), \n",
    "            segments[2], str(segments[1]+0.01), str(segments[2]),\n",
    "            segments[3], str(segments[2]+0.01), str(segments[3]),\n",
    "            segments[4], str(segments[3]+0.01), str(segments[4]),\n",
    "            str(segments[4]+0.01))\n",
    "\n",
    "    df_with_segment = spark.sql(query)\n",
    "    table_name = generate_table_name()\n",
    "    df_with_segment.write.format(\"delta\").mode(\"overwrite\").saveAsTable(table_name)\n",
    "    tables.append(table_name)\n",
    "       \n",
    "    print(\"Step 2\")\n",
    "    # Calculate average fare_amount and tip_amount per segment\n",
    "    segments = [2, 5, 10, 20, 50]  # Adjust these values as per your requirements\n",
    "\n",
    "    query_segment = \"\"\"\n",
    "    SELECT *,\n",
    "    CASE\n",
    "        WHEN trip_distance IS NULL THEN NULL\n",
    "        WHEN trip_distance <= {} THEN \"<= {}\"\n",
    "        WHEN trip_distance <= {} THEN \"{} - {}\"\n",
    "        WHEN trip_distance <= {} THEN \"{} - {}\"\n",
    "        WHEN trip_distance <= {} THEN \"{} - {}\"\n",
    "        WHEN trip_distance <= {} THEN \"{} - {}\"\n",
    "        ELSE \">= {}\"\n",
    "    END AS distance_segment\n",
    "    FROM Bronze.true_big_table\n",
    "    \"\"\".format(segments[0], segments[0], \n",
    "            segments[1], str(segments[0]+0.01), str(segments[1]), \n",
    "            segments[2], str(segments[1]+0.01), str(segments[2]),\n",
    "            segments[3], str(segments[2]+0.01), str(segments[3]),\n",
    "            segments[4], str(segments[3]+0.01), str(segments[4]),\n",
    "            str(segments[4]+0.01))\n",
    "\n",
    "    df_with_segment = spark.sql(query_segment)\n",
    "    df_with_segment.count()    \n",
    "    table_name = generate_table_name()\n",
    "    df_with_segment.write.format(\"delta\").mode(\"overwrite\").saveAsTable(table_name)\n",
    "    tables.append(table_name)\n",
    "    \n",
    "\n",
    "    # df_with_segment.write.format(\"delta\").mode(\"append\").saveAsTable(\"Bronze.segmented_table\")\n",
    "\n",
    "    query_avg = \"\"\"\n",
    "    SELECT distance_segment, \n",
    "        AVG(fare_amount) AS avg_fare_amount, \n",
    "        AVG(tip_amount) AS avg_tip_amount\n",
    "    FROM Bronze.segmented_table\n",
    "    GROUP BY distance_segment\n",
    "    \"\"\"\n",
    "\n",
    "    avg_amounts_by_segment = spark.sql(query_avg)\n",
    "\n",
    "    # Display the result\n",
    "    avg_amounts_by_segment.count()\n",
    "    table_name = generate_table_name()\n",
    "    avg_amounts_by_segment.write.format(\"delta\").mode(\"overwrite\").saveAsTable(table_name)\n",
    "    tables.append(table_name)\n",
    "\n",
    "\n",
    "\n",
    "    # Define the segments for trip_distance\n",
    "    segments = [0, 5, 10, 15, 20, 25]\n",
    "    print(\"Step 3\")\n",
    "    # Calculate average fare_amount, tip_amount, and maximum values per segment\n",
    "    amounts_by_segment = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        distance_segment, \n",
    "        AVG(fare_amount) AS avg_fare_amount, \n",
    "        AVG(tip_amount) AS avg_tip_amount,\n",
    "        MAX(fare_amount) AS max_fare_amount,\n",
    "        MAX(tip_amount) AS max_tip_amount\n",
    "    FROM segmented_table\n",
    "    GROUP BY distance_segment\n",
    "    \"\"\")\n",
    "\n",
    "    amounts_by_segment.count()\n",
    "    table_name = generate_table_name()\n",
    "    amounts_by_segment.write.format(\"delta\").mode(\"overwrite\").saveAsTable(table_name)\n",
    "    tables.append(table_name)\n",
    "\n",
    "    \n",
    "\n",
    "    print(\"Step 4\")\n",
    "    q = spark.sql(\"SELECT COUNT(*) FROM true_big_table\")\n",
    "    table_name = generate_table_name()\n",
    "    adjust_columns_names(q).write.format(\"delta\").mode(\"overwrite\").saveAsTable(table_name)\n",
    "    tables.append(table_name)\n",
    "\n",
    "\n",
    "    print(\"Step 5\")\n",
    "    q = spark.sql(\"SELECT AVG(trip_distance) FROM true_big_table\")\n",
    "    table_name = generate_table_name()\n",
    "    adjust_columns_names(q).write.format(\"delta\").mode(\"overwrite\").saveAsTable(table_name)\n",
    "    tables.append(table_name)\n",
    "\n",
    "    print(\"Step 6\")\n",
    "    q = spark.sql(\"SELECT MAX(fare_amount) FROM true_big_table\")\n",
    "    table_name = generate_table_name()\n",
    "    adjust_columns_names(q).write.format(\"delta\").mode(\"overwrite\").saveAsTable(table_name)\n",
    "    tables.append(table_name)\n",
    "\n",
    "    print(\"Step 7\")\n",
    "    q = spark.sql(\"SELECT VendorID, COUNT(*) AS trip_count FROM true_big_table GROUP BY VendorID\")\n",
    "    table_name = generate_table_name()\n",
    "    adjust_columns_names(q).write.format(\"delta\").mode(\"overwrite\").saveAsTable(table_name)\n",
    "    tables.append(table_name)\n",
    "\n",
    "    print(\"Step 8\")\n",
    "    q = spark.sql(\"SELECT VendorID, AVG(fare_amount) AS avg_fare FROM true_big_table GROUP BY VendorID\")\n",
    "    table_name = generate_table_name()\n",
    "    adjust_columns_names(q).write.format(\"delta\").mode(\"overwrite\").saveAsTable(table_name)\n",
    "    tables.append(table_name)\n",
    "\n",
    "    print(\"Step 9\")\n",
    "    q = spark.sql(\"SELECT VendorID, SUM(tip_amount) AS total_tips FROM true_big_table GROUP BY VendorID\")\n",
    "    table_name = generate_table_name()\n",
    "    adjust_columns_names(q).write.format(\"delta\").mode(\"overwrite\").saveAsTable(table_name)\n",
    "    tables.append(table_name)\n",
    "\n",
    "    print(\"Step 10\")\n",
    "    q = spark.sql(\"SELECT AVG(passenger_count) FROM true_big_table\")\n",
    "    table_name = generate_table_name()\n",
    "    adjust_columns_names(q).write.format(\"delta\").mode(\"overwrite\").saveAsTable(table_name)\n",
    "    tables.append(table_name)\n",
    "\n",
    "    print(\"Step 11\")\n",
    "    q = spark.sql(\"SELECT AVG(total_amount) FROM true_big_table\")\n",
    "    table_name = generate_table_name()\n",
    "    adjust_columns_names(q).write.format(\"delta\").mode(\"overwrite\").saveAsTable(table_name)\n",
    "    tables.append(table_name)\n",
    "\n",
    "    print(\"Step 12\")\n",
    "    q = spark.sql(\"SELECT payment_type, COUNT(*) AS trip_count FROM true_big_table GROUP BY payment_type\")\n",
    "    table_name = generate_table_name()\n",
    "    adjust_columns_names(q).write.format(\"delta\").mode(\"overwrite\").saveAsTable(table_name)\n",
    "    tables.append(table_name)\n",
    "\n",
    "    print(\"Step 12\")\n",
    "    q = spark.sql(\"SELECT RateCodeID, AVG(fare_amount) AS avg_fare, AVG(tip_amount) AS avg_tip FROM true_big_table GROUP BY RateCodeID\")\n",
    "    table_name = generate_table_name()\n",
    "    adjust_columns_names(q).write.format(\"delta\").mode(\"overwrite\").saveAsTable(table_name)\n",
    "    tables.append(table_name)\n",
    "\n",
    "    print(\"Step 13\")\n",
    "    q = spark.sql(\"SELECT passenger_count, COUNT(*) AS trip_count FROM true_big_table GROUP BY passenger_count\")\n",
    "    table_name = generate_table_name()\n",
    "    adjust_columns_names(q).write.format(\"delta\").mode(\"overwrite\").saveAsTable(table_name)\n",
    "    tables.append(table_name)\n",
    "\n",
    "    print(\"Step 14\")\n",
    "    q = spark.sql(\"\"\"\n",
    "    SELECT HOUR(tpep_pickup_datetime) AS pickup_hour, COUNT(*) AS trip_count\n",
    "    FROM true_big_table\n",
    "    GROUP BY pickup_hour\n",
    "    ORDER BY pickup_hour\n",
    "    \"\"\")\n",
    "    table_name = generate_table_name()\n",
    "    adjust_columns_names(q).write.format(\"delta\").mode(\"overwrite\").saveAsTable(table_name)\n",
    "    tables.append(table_name)\n",
    "\n",
    "\n",
    "    print(\"Step 15\")\n",
    "    q = spark.sql(\"\"\"\n",
    "    SELECT VendorID, AVG(fare_amount) AS avg_fare \n",
    "    FROM true_big_table \n",
    "    GROUP BY VendorID \n",
    "    ORDER BY avg_fare DESC\n",
    "    \"\"\")\n",
    "    table_name = generate_table_name()\n",
    "    adjust_columns_names(q).write.format(\"delta\").mode(\"overwrite\").saveAsTable(table_name)\n",
    "    tables.append(table_name)\n",
    "\n",
    "\n",
    "    print(\"Step 16\")\n",
    "    q = spark.sql(\"\"\"\n",
    "    SELECT CORR(trip_distance, fare_amount) AS correlation\n",
    "    FROM true_big_table\n",
    "    \"\"\")\n",
    "    table_name = generate_table_name()\n",
    "    adjust_columns_names(q).write.format(\"delta\").mode(\"overwrite\").saveAsTable(table_name)\n",
    "    tables.append(table_name)\n",
    "\n",
    "\n",
    "    print(\"Step 17\")\n",
    "    q = spark.sql(\"\"\"\n",
    "    SELECT payment_type, AVG(passenger_count) AS avg_passenger_count\n",
    "    FROM true_big_table\n",
    "    GROUP BY payment_type\n",
    "    \"\"\")\n",
    "    table_name = generate_table_name()\n",
    "    adjust_columns_names(q).write.format(\"delta\").mode(\"overwrite\").saveAsTable(table_name)\n",
    "    tables.append(table_name)\n",
    "\n",
    "\n",
    "    print(\"Step 18\")\n",
    "    q = spark.sql(\"\"\"\n",
    "    SELECT PERCENTILE(trip_distance, 0.5) AS median_trip_distance\n",
    "    FROM true_big_table\n",
    "    \"\"\")\n",
    "    table_name = generate_table_name()\n",
    "    adjust_columns_names(q).write.format(\"delta\").mode(\"overwrite\").saveAsTable(table_name)\n",
    "    tables.append(table_name)\n",
    "\n",
    "    print(\"Step 19\")\n",
    "    q = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        sub.VendorID, \n",
    "        sub.avg_fare\n",
    "    FROM \n",
    "        (SELECT \n",
    "            VendorID, \n",
    "            AVG(fare_amount) AS avg_fare \n",
    "        FROM true_big_table \n",
    "        GROUP BY VendorID) AS sub\n",
    "    WHERE \n",
    "        sub.avg_fare > (SELECT AVG(fare_amount) FROM true_big_table)\n",
    "    ORDER BY \n",
    "        sub.avg_fare DESC \n",
    "    \"\"\")\n",
    "    table_name = generate_table_name()\n",
    "    adjust_columns_names(q).write.format(\"delta\").mode(\"overwrite\").saveAsTable(table_name)\n",
    "    tables.append(table_name)\n",
    "\n",
    "\n",
    "    print(\"Step 20\")\n",
    "    q = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        VendorID, \n",
    "        RateCodeID, \n",
    "        AVG(tip_amount) AS avg_tip \n",
    "    FROM true_big_table \n",
    "    GROUP BY \n",
    "        VendorID, RateCodeID \n",
    "    ORDER BY \n",
    "        avg_tip DESC \n",
    "    \"\"\")\n",
    "    table_name = generate_table_name()\n",
    "    adjust_columns_names(q).write.format(\"delta\").mode(\"overwrite\").saveAsTable(table_name)\n",
    "    tables.append(table_name)\n",
    "    \n",
    "    print(\"Step 21\")\n",
    "    q = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        tpep_pickup_datetime, \n",
    "        fare_amount, \n",
    "        AVG(fare_amount) OVER (ORDER BY DATE(tpep_pickup_datetime) ROWS BETWEEN 2 PRECEDING AND CURRENT ROW) AS moving_average\n",
    "    FROM true_big_table \n",
    "    ORDER BY tpep_pickup_datetime\n",
    "    LIMIT 1\n",
    "    \"\"\")\n",
    "    table_name = generate_table_name()\n",
    "    adjust_columns_names(q).write.format(\"delta\").mode(\"overwrite\").saveAsTable(table_name)\n",
    "    tables.append(table_name)\n",
    "\n",
    "    print(\"Step 22\")\n",
    "    q = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        VendorID, \n",
    "        DATE(tpep_pickup_datetime) AS date, \n",
    "        SUM(total_amount) AS total_daily_amount \n",
    "    FROM true_big_table \n",
    "    GROUP BY \n",
    "        VendorID, \n",
    "        date\n",
    "    ORDER BY \n",
    "        VendorID, \n",
    "        date\n",
    "    \"\"\")\n",
    "\n",
    "    table_name = generate_table_name()\n",
    "    adjust_columns_names(q).write.format(\"delta\").mode(\"overwrite\").saveAsTable(table_name)\n",
    "    tables.append(table_name)\n",
    "\n",
    "    print(\"Step 23\")\n",
    "    q = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        VendorID,\n",
    "        SUM(total_amount) AS total_amount\n",
    "    FROM \n",
    "        (SELECT \n",
    "            *, \n",
    "            PERCENT_RANK() OVER(ORDER BY fare_amount DESC) AS rank\n",
    "        FROM true_big_table) AS sub \n",
    "    WHERE \n",
    "        rank <= 0.10\n",
    "    GROUP BY \n",
    "        VendorID\n",
    "    ORDER BY \n",
    "        total_amount DESC\n",
    "    LIMIT 1\n",
    "    \"\"\")\n",
    "\n",
    "    table_name = generate_table_name()\n",
    "    adjust_columns_names(q).write.format(\"delta\").mode(\"overwrite\").saveAsTable(table_name)\n",
    "    tables.append(table_name)\n",
    "\n",
    "\n",
    "    print(\"Step 24\")\n",
    "    q = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        passenger_count, \n",
    "        AVG(trip_distance) AS avg_trip_distance \n",
    "    FROM true_big_table \n",
    "    WHERE fare_amount > (SELECT AVG(fare_amount) FROM true_big_table) \n",
    "    GROUP BY \n",
    "        passenger_count\n",
    "    ORDER BY \n",
    "        passenger_count\n",
    "    \"\"\")\n",
    "\n",
    "    table_name = generate_table_name()\n",
    "    adjust_columns_names(q).write.format(\"delta\").mode(\"overwrite\").saveAsTable(table_name)\n",
    "    tables.append(table_name)\n",
    "\n",
    "    time.sleep(200) if pause else time.sleep(1)\n",
    "\n",
    "    clean_tables(tables)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "queries(pause=False)\n",
    "clean_tables()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  }
 ]
}
